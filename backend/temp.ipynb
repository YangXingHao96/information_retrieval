{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1131)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import snscrape.modules.twitter as twitter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pysolr\n",
    "import uuid\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "def crawl_from_twitter(cnt, query_term):\n",
    "    sent_analyser = SentimentIntensityAnalyzer()\n",
    "    tweets = twitter.TwitterSearchScraper(query_term).get_items()\n",
    "    sliced_scraped_tweets = itertools.islice(tweets, 10)\n",
    "    solr = pysolr.Solr(\n",
    "        'http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "    tweets_to_add = []\n",
    "    for tweet in sliced_scraped_tweets:\n",
    "        sentiment = 0\n",
    "        if sent_analyser.polarity_scores(tweet.rawContent)[\"compound\"]>=0:\n",
    "            sentiment = 1\n",
    "        else:\n",
    "            sentiment = 0\n",
    "        temp = {\n",
    "            \"body\": [tweet.rawContent],\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"like_num\": tweet.likeCount,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"ticker_symbol\": query_term,\n",
    "            \"tweet_id\": tweet.id,\n",
    "            \"post_date\": str(tweet.date),\n",
    "            \"retweet_num\": tweet.retweetCount,\n",
    "            \"writer\": tweet.user.username,\n",
    "        }\n",
    "        tweets_to_add.append(temp)\n",
    "    solr.add(tweets_to_add)\n",
    "    return \n",
    "crawl_from_twitter(10, \"tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   b\n",
       "0  0  -1\n",
       "1  1   0\n",
       "2  2   1\n",
       "3  3   2\n",
       "4  4   3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['A','b'])\n",
    "\n",
    "for i in range(5):\n",
    "    new_row = pd.Series({'A': i, 'b': i-1})\n",
    "    df=pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , parallel_backend, register_parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import contractions\n",
    "import textstat\n",
    "import emoji as Emoji\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')\n",
    "stop = stopwords.words('english')\n",
    "# Parallel processing\n",
    "\n",
    "\n",
    "class SubPreprocessor:\n",
    "    def __init__(self, stemmer=PorterStemmer()):\n",
    "        self.stemmer = stemmer\n",
    "        #self.tokenize = tokenize\n",
    "        self.stop_words = stop\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        res = Parallel(n_jobs=-1)(\n",
    "            delayed(self.processRow)(row[0]) for row in X.loc[:, 'raw_text']\n",
    "        )\n",
    "        res = pd.DataFrame(res, index=X.index)\n",
    "        res = pd.concat([X, res], axis=1)\n",
    "        return res\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def processRow(self, text):\n",
    "        text, urls = self.extractLink(text, '')\n",
    "        text, emojis = self.extractEmoji(text, '')\n",
    "        text, emoticons = self.extractEmoticons(text, '')\n",
    "        text = self.removePunctuation(text)\n",
    "        text = self.removeRtLink(text)\n",
    "        text = self.removeStopWords(text)\n",
    "\n",
    "        return {\n",
    "            #             'url_cnt': len(urls),\n",
    "            #             'emoticons': emoticons,\n",
    "            #             'emojis': emojis ,\n",
    "            #             'emo_cnt': len(emoticons) + sum(emojis.values()),\n",
    "            'clean_text_no_stem_user': text,\n",
    "        }\n",
    "\n",
    "    def removePunctuation(self, text, replace='', remove_num=False, remove_emoji=False):\n",
    "        r_emoji = '\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff]'\n",
    "        r = f'[!#\"$%&\\'()*+,-./:;<=>?@[\\]^_`{{|}}~‚Äî]'\n",
    "        if remove_emoji:\n",
    "            r += f'|[{r_emoji}]'\n",
    "        if remove_num:\n",
    "            r += f'|[0-9]'\n",
    "        text = re.sub(r, replace, text)\n",
    "        return text\n",
    "\n",
    "    def extractLink(self, text, replace_text=''):\n",
    "        #r = '(http\\S+?|www.\\S+?)(?=\\'|\\\")'\n",
    "        #r = '(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "        #r = '''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))'''\n",
    "        #r ='^https?:\\/\\/.*[\\r\\n]*'\n",
    "        r = 'http://\\S+|https://\\S+'\n",
    "        # .apply(lambda url: url[1:-1]) # trim quotes\n",
    "        urls = re.findall(r, text)\n",
    "        text = re.sub(r, replace_text, text)\n",
    "        return text, urls\n",
    "\n",
    "    def extractEmoticons(self, text, replace_text=''):\n",
    "        r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "        emoticons = re.findall(r, text)\n",
    "        text = re.sub(r, replace_text, text)\n",
    "        # replace('-','') removes nose of emoticons\n",
    "        emoticons = [emoticon.replace('-', '') for emoticon in emoticons]\n",
    "        return text, emoticons\n",
    "\n",
    "    def extractEmoji(self, text, replace_text=''):\n",
    "        distinct_ls = Emoji.distinct_emoji_list(text)\n",
    "        emoji_cnt = dict(zip(distinct_ls, [0 for i in distinct_ls]))\n",
    "        for emoji in emoji_cnt.keys():\n",
    "            emoji_cnt[emoji] = text.count(emoji)\n",
    "            text = text.replace(emoji, replace_text)\n",
    "\n",
    "        return text, emoji_cnt\n",
    "\n",
    "    def wordCount(self, text):\n",
    "        return textstat.lexicon_count(text, removepunct=True)\n",
    "\n",
    "    def splitWords(self, text):\n",
    "        w_list = re.split('\\s+', text.strip())\n",
    "        return w_list\n",
    "\n",
    "    def removeRtLink(self, text):\n",
    "        # Removes RT\n",
    "        #text = re.sub('RT @\\w+: ','', text)\n",
    "        #text = text.lower()\n",
    "\n",
    "        # Removes @username from the tweet\n",
    "        #text = re.sub(r'(@[A-Za-z0-9_]+)', '', text)\n",
    "\n",
    "        # Removes link\n",
    "        text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "\n",
    "        # Only considers string or digits or whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Removes digits\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "\n",
    "    def removeStopWords(self, text):\n",
    "        #text_tokens = self.tokenize(text)\n",
    "        text_tokens = word_tokenize(text)\n",
    "        text = [word for word in text_tokens if not word in self.stop_words]\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.processRow(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>post_date</th>\n",
       "      <th>author</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>like_num</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text_no_stem_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642345902303219713</td>\n",
       "      <td>apple</td>\n",
       "      <td>2023-04-02 01:59:11+00:00</td>\n",
       "      <td>atomiccanonAPPL</td>\n",
       "      <td>[„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„ÇìÔºÅÔºÅÔºÅ]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„Çì</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id company                  post_date           author   \n",
       "0  1642345902303219713   apple  2023-04-02 01:59:11+00:00  atomiccanonAPPL  \\\n",
       "\n",
       "                raw_text like_num subjectivity sentiment   \n",
       "0  [„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„ÇìÔºÅÔºÅÔºÅ]        0            0         1  \\\n",
       "\n",
       "  clean_text_no_stem_user  \n",
       "0        „Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„Çì  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['id', 'company', 'post_date', 'author',\n",
    "                           'raw_text', 'like_num', 'subjectivity', 'sentiment'])\n",
    "temp = {'id': 1642345902303219713, 'company': 'apple', 'post_date': '2023-04-02 01:59:11+00:00',\n",
    "        'author': 'atomiccanonAPPL', 'raw_text': ['„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„ÇìÔºÅÔºÅÔºÅ'], 'like_num': 0, 'subjectivity': 0, 'sentiment': 1}\n",
    "df = pd.concat(\n",
    "    [df, pd.DataFrame([temp], columns=df.columns)], ignore_index=True)\n",
    "prep = SubPreprocessor()\n",
    "df = prep.transform(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "\n",
    "# load the model from file\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob \n",
    "def is_subjective(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    if subjectivity > 0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "\n",
    "    if sentiment_score < 0:\n",
    "        return -1\n",
    "    elif sentiment_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "print(is_subjective(\"BOT LOVE IS READY FOR WORLDWIDE LAUNCH starting tomorrow at 6:30AM pst! \\U0001faf0ü§ñüíó \\n\\nYou all are going absolutely love it!üî• \\n\\nGo Tesla Bot! ‚ö°Ô∏èüëä‚ö°Ô∏è \\n\\n$TSLA https://t.co/8oPD5VRNMv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 'company:AAPL AND subjectivity:1', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'q': 'company:AAPL AND subjectivity:0', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'q': 'company:AAPL AND sentiment:0', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'total': 892, 'subjective': 440, 'non_subjective': 452, 'negative': 111, 'neutral': 576, 'positive': 205, 'non_subjective_negative': 1, 'non_subjective_neutral': 441, 'non_subjective_positive': 10, 'subjective_negative': 110, 'subjective_neutral': 135, 'subjective_positive': 195}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pysolr\n",
    "solr = pysolr.Solr(\n",
    "    'http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "start_date = datetime.datetime(2023, 1, 1)\n",
    "end_date = datetime.datetime(2023, 3, 30)\n",
    "# Convert the dates to Solr date format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "range_query = 'post_date:[{} TO {}]'.format(start_date_str, end_date_str)\n",
    "t = \"company:AAPL\"\n",
    "normal_params = {\n",
    "    'q': t,\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**normal_params)\n",
    "total_tweets = results.hits\n",
    "\n",
    "subjective_params = {\n",
    "    'q': t + \" AND subjectivity:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "print(subjective_params)\n",
    "results = solr.search(**subjective_params)\n",
    "subjective_tweets = results.hits\n",
    "\n",
    "non_subjective_params = {\n",
    "    'q': t + \" AND subjectivity:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "print(non_subjective_params)\n",
    "results = solr.search(**non_subjective_params)\n",
    "non_subjective_tweets = results.hits\n",
    "\n",
    "neutral_sentiment_params = {\n",
    "    'q': t + \" AND sentiment:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "print(neutral_sentiment_params)\n",
    "results = solr.search(**neutral_sentiment_params)\n",
    "neutral_tweets = results.hits\n",
    "\n",
    "positive_sentiment_params = {\n",
    "    'q': t + \" AND sentiment:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "\n",
    "results = solr.search(**positive_sentiment_params)\n",
    "positive_tweets = results.hits\n",
    "\n",
    "negative_tweets = total_tweets - positive_tweets - neutral_tweets\n",
    "\n",
    "non_sub_neu_params = {\n",
    "    'q': t + \" AND subjectivity:0 AND sentiment:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**non_sub_neu_params)\n",
    "non_subjective_neutral = results.hits\n",
    "\n",
    "non_sub_pos_params = {\n",
    "    'q': t + \" AND subjectivity:0 AND sentiment:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**non_sub_pos_params)\n",
    "non_subjective_positive = results.hits\n",
    "non_subjective_negative = non_subjective_tweets - non_subjective_positive - non_subjective_neutral\n",
    "\n",
    "sub_neu_params = {\n",
    "    'q': t + \" AND subjectivity:1 AND sentiment:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**sub_neu_params)\n",
    "subjective_neutral = results.hits\n",
    "\n",
    "sub_pos_params = {\n",
    "    'q': t + \" AND subjectivity:1 AND sentiment:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**sub_pos_params)\n",
    "subjective_positive = results.hits\n",
    "\n",
    "subjective_negative = subjective_tweets - subjective_positive - subjective_neutral\n",
    "\n",
    "response = {\n",
    "    \"total\":total_tweets,\n",
    "    \"subjective\": subjective_tweets,\n",
    "    \"non_subjective\": non_subjective_tweets,\n",
    "    \"negative\": negative_tweets,\n",
    "    \"neutral\": neutral_tweets,\n",
    "    \"positive\": positive_tweets,\n",
    "    \"non_subjective_negative\": non_subjective_negative,\n",
    "    \"non_subjective_neutral\": non_subjective_neutral,\n",
    "    \"non_subjective_positive\": non_subjective_positive,\n",
    "    \"subjective_negative\": subjective_negative,\n",
    "    \"subjective_neutral\": subjective_neutral,\n",
    "    \"subjective_positive\": subjective_positive\n",
    "}\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love tim cook']\n",
      "{'id': '1617303984942813184', 'company': ['TSLA'], 'post_date': '2023-01-22T23:31:32Z', 'author': ['EvasTeslaSPlaid'], 'raw_text': ['@cvalente28 Sorry. It‚Äôs done. I love $TSLA'], 'like_num': [64], 'subjectivity': [1], 'sentiment': [0], 'clean_text': 'cvalente28 Sorry Its done I love TSLA', '_version_': 1762527907934109696}\n",
      "{'id': '1616793457481383936', 'company': ['GOOGL'], 'post_date': '2023-01-21T13:42:53Z', 'author': ['moyheen'], 'raw_text': ['Sending love to all my friends at the $goog ‚ù§Ô∏è'], 'like_num': [44], 'subjectivity': [1], 'sentiment': [0], 'clean_text': 'Sending love friends goog', '_version_': 1762527907537747972}\n",
      "{'id': '1625622754690048000', 'company': ['AAPL'], 'post_date': '2023-02-14T22:27:21Z', 'author': ['sparkle6193920'], 'raw_text': ['Tomorrow $aapl gonna get sent üöÄ \\n\\nLove ‚ù§Ô∏è ~ Your Uncle Buffet'], 'like_num': [55], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'Tomorrow aapl gon na get sent Love Your Uncle Buffet', '_version_': 1762527907170746370}\n",
      "{'id': '1618269588042711040', 'company': ['AAPL'], 'post_date': '2023-01-25T15:28:30Z', 'author': ['eWhispers'], 'raw_text': [\"Don't worry... analysts still love $AAPL \\n\\nhttps://t.co/JMP9aM7nbR https://t.co/9QrmRGAHt8\"], 'like_num': [42], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'Dont worry analysts still love AAPL', '_version_': 1762527907078471681}\n",
      "{'id': '1609678748609314816', 'company': ['TSLA'], 'post_date': '2023-01-01T22:31:34Z', 'author': ['EveryoneSaysHi1'], 'raw_text': ['Never met her but I‚Äôm falling in love with this woman. $tsla'], 'like_num': [30], 'subjectivity': [1], 'sentiment': [0], 'clean_text': 'Never met Im falling love woman tsla', '_version_': 1762527907880632327}\n",
      "{'id': '1623825155255566336', 'company': ['TSLA'], 'post_date': '2023-02-09T23:24:20Z', 'author': ['Teslaconomics'], 'raw_text': ['BOT LOVE IS READY FOR WORLDWIDE LAUNCH starting tomorrow at 6:30AM pst! \\U0001faf0ü§ñüíó \\n\\nYou all are going absolutely love it!üî• \\n\\nGo Tesla Bot! ‚ö°Ô∏èüëä‚ö°Ô∏è \\n\\n$TSLA https://t.co/8oPD5VRNMv'], 'like_num': [42], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'BOT LOVE IS READY FOR WORLDWIDE LAUNCH starting tomorrow 630AM pst You going absolutely love Go Tesla Bot TSLA', '_version_': 1762527907982344203}\n",
      "{'id': '1623788887763025920', 'company': ['GOOGL'], 'post_date': '2023-02-09T21:00:14Z', 'author': ['TheRealDrip2Rip'], 'raw_text': [\"$GOOG y'all quiet when you winnin...what up.\\nLove ya! See you tomorrow!\"], 'like_num': [24], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'GOOG yall quiet winninwhat Love ya See tomorrow', '_version_': 1762527907609051138}\n",
      "{'id': '1621286281203585024', 'company': ['TSLA'], 'post_date': '2023-02-02T23:15:46Z', 'author': ['Teslaconomics'], 'raw_text': ['Thanks everyone for all your love and support during this journey. üíï \\n\\n$TSLA'], 'like_num': [58], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'Thanks everyone love support journey TSLA', '_version_': 1762527907969761280}\n",
      "{'id': '1610769030008438784', 'company': ['MSFT'], 'post_date': '2023-01-04T22:43:58Z', 'author': ['Iamraylin_nyc'], 'raw_text': ['I will buy more $msft under $200‚Ä¶already have a good allocation, but LOVE this one'], 'like_num': [53], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'I buy msft 200already good allocation LOVE one', '_version_': 1762527907684548612}\n",
      "{'id': '1621547735865327616', 'company': ['MSFT'], 'post_date': '2023-02-03T16:34:41Z', 'author': ['tradertvshawn'], 'raw_text': ['I love trading, I am sure you can tell.\\n\\nHowever I really love it when Mr. Softy is moving baby!!! \\n$MSFT long off $262 many times, now lets hold for a $264 break!! we started the long at $261.... https://t.co/vQ2LYDgpKY'], 'like_num': [24], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'I love trading I sure tell However I really love Mr Softy moving baby MSFT long 262 many times lets hold 264 break started long 261', '_version_': 1762527907792551939}\n",
      "{'id': '1628038691691327488', 'company': ['GOOGL'], 'post_date': '2023-02-21T14:27:26Z', 'author': ['jj_buckner'], 'raw_text': ['I love a morning when I wake up and $GOOGL is trading lower. Getting closer to 100 shares!'], 'like_num': [27], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'I love morning I wake GOOGL trading lower Getting closer 100 shares', '_version_': 1762527907644702722}\n",
      "{'id': '1619071734627311616', 'company': ['AMZN'], 'post_date': '2023-01-27T20:35:57Z', 'author': ['PatrickWalker56'], 'raw_text': ['$AMZN  study. \\nBEFORE AND AFTER.\\nWhy we love Clean &amp; Simple bases!\\nVIPS in it.\\n#stocks #investing https://t.co/H6VQ0lTwrd'], 'like_num': [28], 'subjectivity': [0], 'sentiment': [0], 'clean_text': 'AMZN study BEFORE AND AFTER Why love Clean amp Simple bases VIPS stocks investing', '_version_': 1762527907380461569}\n",
      "{'id': '1624188342215782400', 'company': ['TSLA'], 'post_date': '2023-02-10T23:27:31Z', 'author': ['Teslaconomics'], 'raw_text': ['Top 2 selling colors of BOT LOVE design in my Swag Store right now.\\U0001faf0ü§ñ @elonmusk $TSLA https://t.co/6fDc29Bh1X'], 'like_num': [58], 'subjectivity': [0], 'sentiment': [0], 'clean_text': 'Top 2 selling colors BOT LOVE design Swag Store right elonmusk TSLA', '_version_': 1762527907984441346}\n",
      "{'id': '1619119269849219072', 'company': ['TSLA'], 'post_date': '2023-01-27T23:44:50Z', 'author': ['EvasTeslaSPlaid'], 'raw_text': ['Crazy how many $TSLA shareholders, fans were blaming and turned on Elon @elonmusk and @Tesla a month ago when the stock was down 58%. Today everyone loves $TSLA and Elon. True believe and love is when you love and believe for better or worse. A lesson to learn here $Tesla fans. https://t.co/eGFmmP8V9q'], 'like_num': [391], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'Crazy many TSLA shareholders fans blaming turned Elon elonmusk Tesla month ago stock 58 Today everyone loves TSLA Elon True believe love love believe better worse A lesson learn Tesla fans', '_version_': 1762527907944595460}\n",
      "{'id': '1613976963634573312', 'company': ['AAPL'], 'post_date': '2023-01-13T19:11:09Z', 'author': ['kiantrades'], 'raw_text': ['Longing off early! Hope I helped in any way possible\\n\\n$CVNA $APRN $BBBY $GME $AAPL $SPX so fun this week\\n\\nLove you all'], 'like_num': [52], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'Longing early Hope I helped way possible CVNA APRN BBBY GME AAPL SPX fun week Love', '_version_': 1762527906977808386}\n",
      "{'id': '1622713876600213504', 'company': ['AAPL'], 'post_date': '2023-02-06T21:48:31Z', 'author': ['thisisorlando'], 'raw_text': ['Never thought I‚Äôd hedge my calls on meme stocks with puts on $AAPL üòÇ\\n\\nI love the 2023 stonk market https://t.co/7Z9IzzWVuT'], 'like_num': [41], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'Never thought Id hedge calls meme stocks puts AAPL I love 2023 stonk market', '_version_': 1762527907143483392}\n",
      "{'id': '1628859079111368704', 'company': ['MSFT'], 'post_date': '2023-02-23T20:47:21Z', 'author': ['GerberKawasaki'], 'raw_text': ['I like this idea. #CallofDutyModernWarfare is the best version of the game. They can expand this version forever‚Ä¶ love having resurgence back! $atvi $msft'], 'like_num': [19], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'I like idea CallofDutyModernWarfare best version game They expand version forever love resurgence back atvi msft', '_version_': 1762527907857563656}\n",
      "{'id': '1625599188166647808', 'company': ['GOOGL'], 'post_date': '2023-02-14T20:53:43Z', 'author': ['compound248'], 'raw_text': [\"If you own $GOOG, what's your valuation thesis?\\n\\nI want to love it, but struggle to believe an attractive return, much less a sexy one.\"], 'like_num': [41], 'subjectivity': [1], 'sentiment': [-1], 'clean_text': 'If GOOG whats valuation thesis I want love struggle believe attractive return much less sexy one', '_version_': 1762527907617439744}\n",
      "{'id': '1620565178566987776', 'company': ['TSLA'], 'post_date': '2023-01-31T23:30:21Z', 'author': ['Teslaconomics'], 'raw_text': ['I love pre-heating my Tesla from anywhere so when I go in my car, it‚Äôs nice and toasty. üî•‚ö°Ô∏è\\n\\n$TSLA\\n\\nhttps://t.co/MsFRoxz2rh'], 'like_num': [88], 'subjectivity': [1], 'sentiment': [1], 'clean_text': 'I love preheating Tesla anywhere I go car nice toasty TSLA', '_version_': 1762527907950886914}\n",
      "{'id': '1628600492770091008', 'company': ['AMZN'], 'post_date': '2023-02-23T03:39:50Z', 'author': ['dturner555'], 'raw_text': ['$AMZN daily\\nI love this setup. May have one more candle down, but price is getting tight in the wedge.\\n#AMZN #qqq #technicalanalysis https://t.co/yOm62ykL28'], 'like_num': [23], 'subjectivity': [1], 'sentiment': [0], 'clean_text': 'AMZN daily I love setup May one candle price getting tight wedge AMZN qqq technicalanalysis', '_version_': 1762527907477979136}\n"
     ]
    }
   ],
   "source": [
    "from pysolr import Solr\n",
    "solr = Solr('http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "query = solr.search(\"raw_text:love tim cokk\", **{'rows': 20})\n",
    "res = []\n",
    "for i,v in enumerate(query.spellcheck[\"collations\"]):\n",
    "    if i % 2 == 0:\n",
    "        continue\n",
    "    else:\n",
    "        s = v.replace(\"raw_text:\", \"\")\n",
    "        res.append(s)\n",
    "print(res)\n",
    "\n",
    "for res in query:\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

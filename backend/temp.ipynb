{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1131)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import snscrape.modules.twitter as twitter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pysolr\n",
    "import uuid\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "def crawl_from_twitter(cnt, query_term):\n",
    "    sent_analyser = SentimentIntensityAnalyzer()\n",
    "    tweets = twitter.TwitterSearchScraper(query_term).get_items()\n",
    "    sliced_scraped_tweets = itertools.islice(tweets, 10)\n",
    "    solr = pysolr.Solr(\n",
    "        'http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "    tweets_to_add = []\n",
    "    for tweet in sliced_scraped_tweets:\n",
    "        sentiment = 0\n",
    "        if sent_analyser.polarity_scores(tweet.rawContent)[\"compound\"]>=0:\n",
    "            sentiment = 1\n",
    "        else:\n",
    "            sentiment = 0\n",
    "        temp = {\n",
    "            \"body\": [tweet.rawContent],\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"like_num\": tweet.likeCount,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"ticker_symbol\": query_term,\n",
    "            \"tweet_id\": tweet.id,\n",
    "            \"post_date\": str(tweet.date),\n",
    "            \"retweet_num\": tweet.retweetCount,\n",
    "            \"writer\": tweet.user.username,\n",
    "        }\n",
    "        tweets_to_add.append(temp)\n",
    "    solr.add(tweets_to_add)\n",
    "    return \n",
    "crawl_from_twitter(10, \"tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   b\n",
       "0  0  -1\n",
       "1  1   0\n",
       "2  2   1\n",
       "3  3   2\n",
       "4  4   3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['A','b'])\n",
    "\n",
    "for i in range(5):\n",
    "    new_row = pd.Series({'A': i, 'b': i-1})\n",
    "    df=pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , parallel_backend, register_parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import contractions\n",
    "import textstat\n",
    "import emoji as Emoji\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')\n",
    "stop = stopwords.words('english')\n",
    "# Parallel processing\n",
    "\n",
    "\n",
    "class SubPreprocessor:\n",
    "    def __init__(self, stemmer=PorterStemmer()):\n",
    "        self.stemmer = stemmer\n",
    "        #self.tokenize = tokenize\n",
    "        self.stop_words = stop\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        res = Parallel(n_jobs=-1)(\n",
    "            delayed(self.processRow)(row[0]) for row in X.loc[:, 'raw_text']\n",
    "        )\n",
    "        res = pd.DataFrame(res, index=X.index)\n",
    "        res = pd.concat([X, res], axis=1)\n",
    "        return res\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def processRow(self, text):\n",
    "        text, urls = self.extractLink(text, '')\n",
    "        text, emojis = self.extractEmoji(text, '')\n",
    "        text, emoticons = self.extractEmoticons(text, '')\n",
    "        text = self.removePunctuation(text)\n",
    "        text = self.removeRtLink(text)\n",
    "        text = self.removeStopWords(text)\n",
    "\n",
    "        return {\n",
    "            #             'url_cnt': len(urls),\n",
    "            #             'emoticons': emoticons,\n",
    "            #             'emojis': emojis ,\n",
    "            #             'emo_cnt': len(emoticons) + sum(emojis.values()),\n",
    "            'clean_text_no_stem_user': text,\n",
    "        }\n",
    "\n",
    "    def removePunctuation(self, text, replace='', remove_num=False, remove_emoji=False):\n",
    "        r_emoji = '\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff]'\n",
    "        r = f'[!#\"$%&\\'()*+,-./:;<=>?@[\\]^_`{{|}}~‚Äî]'\n",
    "        if remove_emoji:\n",
    "            r += f'|[{r_emoji}]'\n",
    "        if remove_num:\n",
    "            r += f'|[0-9]'\n",
    "        text = re.sub(r, replace, text)\n",
    "        return text\n",
    "\n",
    "    def extractLink(self, text, replace_text=''):\n",
    "        #r = '(http\\S+?|www.\\S+?)(?=\\'|\\\")'\n",
    "        #r = '(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "        #r = '''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))'''\n",
    "        #r ='^https?:\\/\\/.*[\\r\\n]*'\n",
    "        r = 'http://\\S+|https://\\S+'\n",
    "        # .apply(lambda url: url[1:-1]) # trim quotes\n",
    "        urls = re.findall(r, text)\n",
    "        text = re.sub(r, replace_text, text)\n",
    "        return text, urls\n",
    "\n",
    "    def extractEmoticons(self, text, replace_text=''):\n",
    "        r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "        emoticons = re.findall(r, text)\n",
    "        text = re.sub(r, replace_text, text)\n",
    "        # replace('-','') removes nose of emoticons\n",
    "        emoticons = [emoticon.replace('-', '') for emoticon in emoticons]\n",
    "        return text, emoticons\n",
    "\n",
    "    def extractEmoji(self, text, replace_text=''):\n",
    "        distinct_ls = Emoji.distinct_emoji_list(text)\n",
    "        emoji_cnt = dict(zip(distinct_ls, [0 for i in distinct_ls]))\n",
    "        for emoji in emoji_cnt.keys():\n",
    "            emoji_cnt[emoji] = text.count(emoji)\n",
    "            text = text.replace(emoji, replace_text)\n",
    "\n",
    "        return text, emoji_cnt\n",
    "\n",
    "    def wordCount(self, text):\n",
    "        return textstat.lexicon_count(text, removepunct=True)\n",
    "\n",
    "    def splitWords(self, text):\n",
    "        w_list = re.split('\\s+', text.strip())\n",
    "        return w_list\n",
    "\n",
    "    def removeRtLink(self, text):\n",
    "        # Removes RT\n",
    "        #text = re.sub('RT @\\w+: ','', text)\n",
    "        #text = text.lower()\n",
    "\n",
    "        # Removes @username from the tweet\n",
    "        #text = re.sub(r'(@[A-Za-z0-9_]+)', '', text)\n",
    "\n",
    "        # Removes link\n",
    "        text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "\n",
    "        # Only considers string or digits or whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Removes digits\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "\n",
    "    def removeStopWords(self, text):\n",
    "        #text_tokens = self.tokenize(text)\n",
    "        text_tokens = word_tokenize(text)\n",
    "        text = [word for word in text_tokens if not word in self.stop_words]\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.processRow(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>post_date</th>\n",
       "      <th>author</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>like_num</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text_no_stem_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642345902303219713</td>\n",
       "      <td>apple</td>\n",
       "      <td>2023-04-02 01:59:11+00:00</td>\n",
       "      <td>atomiccanonAPPL</td>\n",
       "      <td>[„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„ÇìÔºÅÔºÅÔºÅ]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„Çì</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id company                  post_date           author   \n",
       "0  1642345902303219713   apple  2023-04-02 01:59:11+00:00  atomiccanonAPPL  \\\n",
       "\n",
       "                raw_text like_num subjectivity sentiment   \n",
       "0  [„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„ÇìÔºÅÔºÅÔºÅ]        0            0         1  \\\n",
       "\n",
       "  clean_text_no_stem_user  \n",
       "0        „Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„Çì  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['id', 'company', 'post_date', 'author',\n",
    "                           'raw_text', 'like_num', 'subjectivity', 'sentiment'])\n",
    "temp = {'id': 1642345902303219713, 'company': 'apple', 'post_date': '2023-04-02 01:59:11+00:00',\n",
    "        'author': 'atomiccanonAPPL', 'raw_text': ['„Ç∞„É¨„Éì„É•„Ç™„Éï„ÅÆ„Ç≥„Çπ„Éó„É¨„Åß„Åç„Çã„ÇÑ„ÇìÔºÅÔºÅÔºÅ'], 'like_num': 0, 'subjectivity': 0, 'sentiment': 1}\n",
    "df = pd.concat(\n",
    "    [df, pd.DataFrame([temp], columns=df.columns)], ignore_index=True)\n",
    "prep = SubPreprocessor()\n",
    "df = prep.transform(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "\n",
    "# load the model from file\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023', '01', '01']\n"
     ]
    }
   ],
   "source": [
    "d1= \"2023-01-01\"\n",
    "t = d1.split(\"-\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1609676719925137408', 'company': ['AAPL'], 'post_date': ['2023-01-01T22:23:31Z'], 'author': ['David_Tracey'], 'raw_text': ['Things are going to get very interesting but when...?\\nMy Q1 forecast below...\\n\\nSome updates here on $AAPL too as requested.\\n\\nhttps://t.co/dU1FcS22JP https://t.co/x8RhRPXHhD'], 'like_num': [27], 'subjectivity': [0], 'sentiment': [0.0], 'clean_text': ['Things going get interesting My Q1 forecast Some updates AAPL requested'], '_version_': 1762388624723148800}\n",
      "{'id': '1609656899674284032', 'company': ['AAPL'], 'post_date': ['2023-01-01T21:04:45Z'], 'author': ['ASTS_Investors'], 'raw_text': [\"Happy New Year SpaceMob! \\n\\nLet's hope 2023 will be the year of #BlueWalker3 and $ASTS üöÄü§†\\n\\n#Space5G #5G $GSAT $T $AMT $TMUS $AAPL $IRDM #SpaceX $VOD https://t.co/cefLRpuvEl\"], 'like_num': [64], 'subjectivity': [1], 'sentiment': [1.0], 'clean_text': ['Happy New Year SpaceMob Lets hope 2023 year BlueWalker3 ASTS Space5G 5G GSAT T AMT TMUS AAPL IRDM SpaceX VOD'], '_version_': 1762388624792354816}\n",
      "{'id': '1609651892447023104', 'company': ['AAPL'], 'post_date': ['2023-01-01T20:44:51Z'], 'author': ['BigCheds'], 'raw_text': ['$AAPL Apple weekly chart - hammer candle with spring from June lows https://t.co/5UTAgINmn2'], 'like_num': [104], 'subjectivity': [0], 'sentiment': [0.0], 'clean_text': ['AAPL Apple weekly chart hammer candle spring June lows'], '_version_': 1762388624797597696}\n",
      "{'id': '1609636954454343680', 'company': ['AAPL'], 'post_date': ['2023-01-01T19:45:30Z'], 'author': ['ThetaWarrior'], 'raw_text': ['$AAPL Yearly -26.8% https://t.co/SewErQmOPE'], 'like_num': [80], 'subjectivity': [0], 'sentiment': [0.0], 'clean_text': ['AAPL Yearly 268'], '_version_': 1762388624797597697}\n",
      "{'id': '1609625505287790592', 'company': ['AAPL'], 'post_date': ['2023-01-01T19:00:00Z'], 'author': ['TrendSpider'], 'raw_text': ['Looking for shorts into the new year? üìâ\\n\\nThis weekly triangle breakdown scan is a great way to generate some potential candidates in seconds!\\n\\n$AAPL $DLTR $CMG $DG $MOS $CF $ALB\\n\\nhttps://t.co/iceKYDRwye'], 'like_num': [28], 'subjectivity': [1], 'sentiment': [1.0], 'clean_text': ['Looking shorts new year This weekly triangle breakdown scan great way generate potential candidates seconds AAPL DLTR CMG DG MOS CF ALB'], '_version_': 1762388624797597698}\n",
      "{'id': '1609585344948887552', 'company': ['AAPL'], 'post_date': ['2023-01-01T16:20:25Z'], 'author': ['lord_fed'], 'raw_text': ['Don‚Äôt really see what is holding $AAPL up @ 20x P/E\\n\\nCould see 107-113 in 2023. Can‚Äôt imagine much further down.'], 'like_num': [149], 'subjectivity': [1], 'sentiment': [-1.0], 'clean_text': ['Dont really see holding AAPL 20x PE Could see 107113 2023 Cant imagine much'], '_version_': 1762388624798646272}\n",
      "{'id': '1609552780863369216', 'company': ['AAPL'], 'post_date': ['2023-01-01T14:11:01Z'], 'author': ['Mr_Derivatives'], 'raw_text': [\"$AAPL A lot of ppl might be surprised if I said in 2023 Apple could see $95. But then again a lot of ppl didn't think TSLA would have crashed that hard either!\\n\\nWe got a nice rounding top formation, and a huge gap fill at $95, which will still be well above pre-covid highs!\\n\\nFWIW https://t.co/8n37VOPyUu\"], 'like_num': [519], 'subjectivity': [1], 'sentiment': [1.0], 'clean_text': ['AAPL A lot ppl might surprised I said 2023 Apple could see 95 But lot ppl didnt think TSLA would crashed hard either We got nice rounding top formation huge gap fill 95 still well precovid highs FWIW'], '_version_': 1762388624798646273}\n",
      "{'id': '1609542582652084224', 'company': ['AAPL'], 'post_date': ['2023-01-01T13:30:30Z'], 'author': ['ProblemSniper'], 'raw_text': ['üìà $AAPL Break of a trendline support on the weekly while the RSI and MACD are in inclined bearish with growing momentum and some bear positioning on it into the earnings cycle (Feb Positions). PT: 115 https://t.co/pkyYXpnLAK'], 'like_num': [105], 'subjectivity': [1], 'sentiment': [1.0], 'clean_text': ['AAPL Break trendline support weekly RSI MACD inclined bearish growing momentum bear positioning earnings cycle Feb Positions PT 115'], '_version_': 1762388624799694848}\n",
      "{'id': '1609407163671400448', 'company': ['AAPL'], 'post_date': ['2023-01-01T04:32:23Z'], 'author': ['The_AI_Investor'], 'raw_text': ['Apple iphone market share peaked in H1 2009 and then falling, the stock should fall too, right? \\n\\nBut no, $AAPL was up 10x in the next 6 years from 2009 to 2015 https://t.co/vE04xsvtiM'], 'like_num': [66], 'subjectivity': [1], 'sentiment': [1.0], 'clean_text': ['Apple iphone market share peaked H1 2009 falling stock fall right But AAPL 10x next 6 years 2009 2015'], '_version_': 1762388624799694849}\n",
      "{'id': '1609341872639512576', 'company': ['AAPL'], 'post_date': ['2023-01-01T00:12:57Z'], 'author': ['NeelyTamminga'], 'raw_text': ['FinTwit: $AAPL üìàüìâ ?\\nMe: mmmm üçéü•ß https://t.co/fRUXuAlVD0'], 'like_num': [52], 'subjectivity': [0], 'sentiment': [0.0], 'clean_text': ['FinTwit AAPL Me mmmm'], '_version_': 1762388624801792000}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "solr = pysolr.Solr(\n",
    "    'http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "start_date = datetime.datetime(2023, 1, 1)\n",
    "end_date = datetime.datetime(2023, 1, 2)\n",
    "# Convert the dates to Solr date format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "range_query = 'post_date:[{} TO {}]'.format(start_date_str, end_date_str)\n",
    "params = {\n",
    "    'q': 'raw_text:AAPL',\n",
    "    'fq': range_query,\n",
    "    'rows': 10,\n",
    "    'sort': 'post_date desc'\n",
    "}\n",
    "\n",
    "results = solr.search(**params)\n",
    "for r in results:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
